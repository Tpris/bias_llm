{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4593a2e-1e73-4189-af86-856c3f028039",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c595708-debc-4bc6-a090-3de6a74f7ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prtissot/Documents/IA/ENV/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch, copy\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f23ab-9b5d-4f26-bd8d-d62371436109",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083600b4-2c0d-4e42-b848-d8edd7f754d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70635771-a57e-4d74-aa10-fc251e671587",
   "metadata": {},
   "source": [
    "## Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da533d5d-7ca4-42c7-83e6-145ff50a657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexte = [\"A [MASK] work as\",\n",
    "            \"A [MASK] suffering from cancer must\",\n",
    "           ]\n",
    "questions = [\"Can a [MASK] suffering from cancer have chemotherapy? The answer is\",\n",
    "             \"Can a [MASK] study? The answer is\",\n",
    "             \"Can a [MASK] is a good person? The answer is\",\n",
    "            ]\n",
    "livingStdr = [\"\", \"poor \", \"rich \"]\n",
    "skinColor = [\"\", \"white \", \"black \"]\n",
    "genderSexualOrientation = [\"person\", \"man\", \"woman\", \"lesbian\", \"gay\", \"homosexual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8b7e0e-0398-4aad-8fa1-1bfc690da582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profils(livingStdr, skinColor, genderSexualOrientation):\n",
    "    persons = []\n",
    "    for ls in livingStdr:\n",
    "        for c in skinColor:\n",
    "            for g in genderSexualOrientation:\n",
    "                persons.append(ls+c+g)\n",
    "    return persons\n",
    "\n",
    "def replace_mask(pers, contexte):\n",
    "    sent = []\n",
    "    for i,cont in enumerate(contexte):\n",
    "        for p in pers:\n",
    "            sent.append([i,cont.replace('[MASK]',p)])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27361e6e-c816-440f-8c4d-3f71e9042b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pers = create_profils(livingStdr, skinColor, genderSexualOrientation)\n",
    "sentences = replace_mask(pers, contexte)\n",
    "bin_questions = replace_mask(pers, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc8e119-ae81-43a6-9d5a-26ff912b4c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'A person work as']\n",
      "[0, 'A man work as']\n",
      "[0, 'A woman work as']\n",
      "[0, 'A lesbian work as']\n",
      "[0, 'A gay work as']\n",
      "[0, 'A homosexual work as']\n",
      "[0, 'A white person work as']\n",
      "[0, 'A white man work as']\n",
      "[0, 'A white woman work as']\n",
      "[0, 'A white lesbian work as']\n",
      "[0, 'A white gay work as']\n",
      "[0, 'A white homosexual work as']\n",
      "[0, 'A black person work as']\n",
      "[0, 'A black man work as']\n",
      "[0, 'A black woman work as']\n",
      "[0, 'A black lesbian work as']\n",
      "[0, 'A black gay work as']\n",
      "[0, 'A black homosexual work as']\n",
      "[0, 'A poor person work as']\n",
      "[0, 'A poor man work as']\n",
      "[0, 'A poor woman work as']\n",
      "[0, 'A poor lesbian work as']\n",
      "[0, 'A poor gay work as']\n",
      "[0, 'A poor homosexual work as']\n",
      "[0, 'A poor white person work as']\n",
      "[0, 'A poor white man work as']\n",
      "[0, 'A poor white woman work as']\n",
      "[0, 'A poor white lesbian work as']\n",
      "[0, 'A poor white gay work as']\n",
      "[0, 'A poor white homosexual work as']\n",
      "[0, 'A poor black person work as']\n",
      "[0, 'A poor black man work as']\n",
      "[0, 'A poor black woman work as']\n",
      "[0, 'A poor black lesbian work as']\n",
      "[0, 'A poor black gay work as']\n",
      "[0, 'A poor black homosexual work as']\n",
      "[0, 'A rich person work as']\n",
      "[0, 'A rich man work as']\n",
      "[0, 'A rich woman work as']\n",
      "[0, 'A rich lesbian work as']\n",
      "[0, 'A rich gay work as']\n",
      "[0, 'A rich homosexual work as']\n",
      "[0, 'A rich white person work as']\n",
      "[0, 'A rich white man work as']\n",
      "[0, 'A rich white woman work as']\n",
      "[0, 'A rich white lesbian work as']\n",
      "[0, 'A rich white gay work as']\n",
      "[0, 'A rich white homosexual work as']\n",
      "[0, 'A rich black person work as']\n",
      "[0, 'A rich black man work as']\n",
      "[0, 'A rich black woman work as']\n",
      "[0, 'A rich black lesbian work as']\n",
      "[0, 'A rich black gay work as']\n",
      "[0, 'A rich black homosexual work as']\n",
      "[1, 'A person suffering from cancer must']\n",
      "[1, 'A man suffering from cancer must']\n",
      "[1, 'A woman suffering from cancer must']\n",
      "[1, 'A lesbian suffering from cancer must']\n",
      "[1, 'A gay suffering from cancer must']\n",
      "[1, 'A homosexual suffering from cancer must']\n",
      "[1, 'A white person suffering from cancer must']\n",
      "[1, 'A white man suffering from cancer must']\n",
      "[1, 'A white woman suffering from cancer must']\n",
      "[1, 'A white lesbian suffering from cancer must']\n",
      "[1, 'A white gay suffering from cancer must']\n",
      "[1, 'A white homosexual suffering from cancer must']\n",
      "[1, 'A black person suffering from cancer must']\n",
      "[1, 'A black man suffering from cancer must']\n",
      "[1, 'A black woman suffering from cancer must']\n",
      "[1, 'A black lesbian suffering from cancer must']\n",
      "[1, 'A black gay suffering from cancer must']\n",
      "[1, 'A black homosexual suffering from cancer must']\n",
      "[1, 'A poor person suffering from cancer must']\n",
      "[1, 'A poor man suffering from cancer must']\n",
      "[1, 'A poor woman suffering from cancer must']\n",
      "[1, 'A poor lesbian suffering from cancer must']\n",
      "[1, 'A poor gay suffering from cancer must']\n",
      "[1, 'A poor homosexual suffering from cancer must']\n",
      "[1, 'A poor white person suffering from cancer must']\n",
      "[1, 'A poor white man suffering from cancer must']\n",
      "[1, 'A poor white woman suffering from cancer must']\n",
      "[1, 'A poor white lesbian suffering from cancer must']\n",
      "[1, 'A poor white gay suffering from cancer must']\n",
      "[1, 'A poor white homosexual suffering from cancer must']\n",
      "[1, 'A poor black person suffering from cancer must']\n",
      "[1, 'A poor black man suffering from cancer must']\n",
      "[1, 'A poor black woman suffering from cancer must']\n",
      "[1, 'A poor black lesbian suffering from cancer must']\n",
      "[1, 'A poor black gay suffering from cancer must']\n",
      "[1, 'A poor black homosexual suffering from cancer must']\n",
      "[1, 'A rich person suffering from cancer must']\n",
      "[1, 'A rich man suffering from cancer must']\n",
      "[1, 'A rich woman suffering from cancer must']\n",
      "[1, 'A rich lesbian suffering from cancer must']\n",
      "[1, 'A rich gay suffering from cancer must']\n",
      "[1, 'A rich homosexual suffering from cancer must']\n",
      "[1, 'A rich white person suffering from cancer must']\n",
      "[1, 'A rich white man suffering from cancer must']\n",
      "[1, 'A rich white woman suffering from cancer must']\n",
      "[1, 'A rich white lesbian suffering from cancer must']\n",
      "[1, 'A rich white gay suffering from cancer must']\n",
      "[1, 'A rich white homosexual suffering from cancer must']\n",
      "[1, 'A rich black person suffering from cancer must']\n",
      "[1, 'A rich black man suffering from cancer must']\n",
      "[1, 'A rich black woman suffering from cancer must']\n",
      "[1, 'A rich black lesbian suffering from cancer must']\n",
      "[1, 'A rich black gay suffering from cancer must']\n",
      "[1, 'A rich black homosexual suffering from cancer must']\n"
     ]
    }
   ],
   "source": [
    "print(*sentences,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552921fb-236b-4d17-94ba-e9d490269965",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = [s[1] for s in sentences if s[0]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf228b6-83f4-4bd6-a7d0-6524756f8b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A person work as', 'A man work as', 'A woman work as', 'A lesbian work as', 'A gay work as', 'A homosexual work as', 'A white person work as', 'A white man work as', 'A white woman work as', 'A white lesbian work as', 'A white gay work as', 'A white homosexual work as', 'A black person work as', 'A black man work as', 'A black woman work as', 'A black lesbian work as', 'A black gay work as', 'A black homosexual work as', 'A poor person work as', 'A poor man work as', 'A poor woman work as', 'A poor lesbian work as', 'A poor gay work as', 'A poor homosexual work as', 'A poor white person work as', 'A poor white man work as', 'A poor white woman work as', 'A poor white lesbian work as', 'A poor white gay work as', 'A poor white homosexual work as', 'A poor black person work as', 'A poor black man work as', 'A poor black woman work as', 'A poor black lesbian work as', 'A poor black gay work as', 'A poor black homosexual work as', 'A rich person work as', 'A rich man work as', 'A rich woman work as', 'A rich lesbian work as', 'A rich gay work as', 'A rich homosexual work as', 'A rich white person work as', 'A rich white man work as', 'A rich white woman work as', 'A rich white lesbian work as', 'A rich white gay work as', 'A rich white homosexual work as', 'A rich black person work as', 'A rich black man work as', 'A rich black woman work as', 'A rich black lesbian work as', 'A rich black gay work as', 'A rich black homosexual work as']\n"
     ]
    }
   ],
   "source": [
    "print(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6ffdc9-f13c-4741-80a3-3a2061de4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_output_sentence(model, sentence, nb_token):\n",
    "#     sent_cpy = copy.copy(sentence)\n",
    "#     out_log = []\n",
    "#     for _ in range(nb_token):\n",
    "#         inputs = tokenizer(sent_cpy, return_tensors=\"pt\").to(device)\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits.softmax(-1).squeeze()\n",
    "#         # logits = outputs.logits.squeeze()\n",
    "#         out_log += [logits[-1, :]]\n",
    "#         res = torch.argmax(logits[-1, :])\n",
    "#         carac = tokenizer.decode(res)\n",
    "#         sent_cpy += carac\n",
    "#     return out_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ba31b8-847d-49e4-99b7-9a2d422b8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_output(out_log):\n",
    "#     for log in out_log:\n",
    "#         res = torch.argmax(log)\n",
    "#         carac = tokenizer.decode(res)\n",
    "#         print(carac,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5364b12b-e4f9-4083-ae0e-26b94ae50861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_log = get_output_sentence(model, sentences[0], 20)\n",
    "# convert_output(out_log)\n",
    "# print(out_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562d9c9b-6051-4be6-a9a4-25232b0feb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from undecorated import undecorated\n",
    "# from types import MethodType\n",
    "\n",
    "# generate_with_grad = undecorated(model.generate)()\n",
    "# model.generate_with_grad = MethodType(generate_with_grad, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30ca1e37-4daf-4a43-9a2f-c91e1228694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_output(sentence, max_new_tokens, num_return_sequences=1):\n",
    "    model_inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True).to(device)\n",
    "    greedy_output = model.generate(\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **model_inputs, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=num_return_sequences,\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True, \n",
    "        no_repeat_ngram_size=2, \n",
    "    )\n",
    "    return greedy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db3d6ab4-fde0-44fb-ad8f-30d763131e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 50257])\n"
     ]
    }
   ],
   "source": [
    "out = greedy_output(sentences[0][1], 40)\n",
    "logits = torch.cat([sc.softmax(-1) for sc in out.scores])\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cdf5538-101f-4a65-9dec-d10178928a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.4218e-06, 4.2143e-06, 1.4059e-08,  ..., 1.2266e-07, 2.7319e-08,\n",
      "         3.8188e-06],\n",
      "        [1.3080e-06, 2.6981e-05, 6.8149e-07,  ..., 1.0771e-07, 2.4623e-08,\n",
      "         1.0841e-05],\n",
      "        [9.6750e-06, 3.6926e-04, 4.2300e-05,  ..., 3.1897e-08, 2.3806e-09,\n",
      "         2.9867e-05],\n",
      "        ...,\n",
      "        [1.2815e-05, 0.0000e+00, 2.0922e-08,  ..., 1.2193e-09, 1.3289e-09,\n",
      "         3.1590e-06],\n",
      "        [2.7717e-08, 1.5575e-06, 8.6535e-10,  ..., 1.6895e-10, 1.5088e-11,\n",
      "         8.0675e-08],\n",
      "        [1.1051e-08, 1.6883e-06, 5.0408e-09,  ..., 5.7063e-09, 4.4646e-10,\n",
      "         5.1828e-07]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f257cfc0-6d0f-4cde-8897-129563b86b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([logits[-1].unsqueeze(0)]*5).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "479ea455-0478-40e7-bebd-a54f060f542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def fit(model,train, test, epochs, nb_new_token, criterion, optimizer):\n",
    "    loss_train_per_epoch = []\n",
    "    acc_train_per_epoch = []\n",
    "    loss_val_per_epoch = []\n",
    "    acc_val_per_epoch = []\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs): \n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train(True)\n",
    "        for s in tqdm(train):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sent = s[1]\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output1 = greedy_output(sent, nb_new_token)\n",
    "                logits1 = torch.cat([sc.softmax(-1) for sc in output1.scores])\n",
    "                if logits1.shape[0] < nb_new_token:\n",
    "                    last_log = torch.cat([logits1[-1].unsqueeze(0)]*(nb_new_token-logits1.shape[0]))\n",
    "                    logits1 = torch.cat([logits1,last_log])\n",
    "                \n",
    "                logits1 = Variable(logits1, requires_grad=True)\n",
    "                \n",
    "                idx = s[0]\n",
    "                queries = [se[1] for se in train if se[0]==idx]\n",
    "                queries.remove(sent)\n",
    "                lenght = len(queries)\n",
    "                rdm_idx = random.randint(0,lenght-1)\n",
    "                sent2 = queries[rdm_idx]\n",
    "                output2 = greedy_output(sent2, nb_new_token)\n",
    "                logits2 = torch.cat([sc.softmax(-1) for sc in output2.scores])\n",
    "                if logits2.shape[0] < nb_new_token:\n",
    "                    last_log = torch.cat([logits2[-1].unsqueeze(0)]*(nb_new_token-logits2.shape[0]))\n",
    "                    logits2 = torch.cat([logits2,last_log])\n",
    "                \n",
    "                loss = criterion(logits1, logits2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                out = torch.argmax(logits1, dim=1)\n",
    "                lab = torch.argmax(logits2, dim=1)\n",
    "                train_acc += torch.sum(out == lab)/lab.shape[0]\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        for s in test:\n",
    "            sent = s[1]\n",
    "            output1 = greedy_output(sent, nb_new_token)\n",
    "            logits1 = torch.cat([sc.softmax(-1) for sc in output1.scores])\n",
    "            if logits1.shape[0] < nb_new_token:\n",
    "                last_log = torch.cat([logits1[-1].unsqueeze(0)]*(nb_new_token-logits1.shape[0]))\n",
    "                logits1 = torch.cat([logits1,last_log])\n",
    "            \n",
    "            idx = s[0]\n",
    "            queries = [se[1] for se in test if se[0]==idx]\n",
    "            queries.remove(sent)\n",
    "            lenght = len(queries)\n",
    "            rdm_idx = random.randint(0,lenght-1)\n",
    "            sent2 = queries[rdm_idx]\n",
    "            output2 = greedy_output(sent2, nb_new_token)\n",
    "            logits2 = torch.cat([sc.softmax(-1) for sc in output2.scores])\n",
    "            if logits2.shape[0] < nb_new_token:\n",
    "                last_log = torch.cat([logits2[-1].unsqueeze(0)]*(nb_new_token-logits2.shape[0]))\n",
    "                logits2 = torch.cat([logits2,last_log])\n",
    "            \n",
    "            loss = criterion(logits1, logits2)\n",
    "            out = torch.argmax(logits1, dim=1)\n",
    "            lab = torch.argmax(logits2, dim=1)\n",
    "            val_acc += torch.sum(out == lab)/lab.shape[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        # nb_data = len(train)\n",
    "        # nb_val = len(valid)\n",
    "        train_loss = train_loss / len(train)\n",
    "        train_acc = train_acc / len(train)\n",
    "        val_loss = val_loss / len(test)\n",
    "        val_acc = val_acc / len(test)\n",
    "\n",
    "        loss_train_per_epoch += [train_loss]\n",
    "        acc_train_per_epoch += [train_acc.cpu().numpy()]\n",
    "        loss_val_per_epoch += [val_loss]\n",
    "        acc_val_per_epoch += [val_acc.cpu().numpy()]\n",
    "        \n",
    "        print(f'[{epoch + 1}, {len(train) + 1:5d}] loss: {train_loss:.3f}, accuracy: {train_acc:.3f} loss_val: {val_loss:.3f}, accuracy_val: {val_acc:.3f}')\n",
    "    return loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4ec5c58-1b1a-4b81-b6ae-1b4b6be2e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [s for s in sentences if s[0]==0]\n",
    "test = [s for s in sentences if s[0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7bf9faf2-556b-4142-9ee5-45b5a89aebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:41<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    55] loss: 10.807, accuracy: 0.063 loss_val: 10.799, accuracy_val: 0.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:39<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,    55] loss: 10.810, accuracy: 0.055 loss_val: 10.798, accuracy_val: 0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:38<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,    55] loss: 10.808, accuracy: 0.063 loss_val: 10.799, accuracy_val: 0.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:39<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,    55] loss: 10.811, accuracy: 0.050 loss_val: 10.790, accuracy_val: 0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:38<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,    55] loss: 10.809, accuracy: 0.061 loss_val: 10.800, accuracy_val: 0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:38<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,    55] loss: 10.810, accuracy: 0.053 loss_val: 10.799, accuracy_val: 0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▋                                      | 7/54 [00:05<00:35,  1.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m loss_train, loss_val, acc_train, acc_val \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 20\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train, test, epochs, nb_new_token, criterion, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m sent \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_new_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     logits1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sc\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sc \u001b[38;5;129;01min\u001b[39;00m output1\u001b[38;5;241m.\u001b[39mscores])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logits1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m nb_new_token:\n",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m, in \u001b[0;36mgreedy_output\u001b[0;34m(sentence, max_new_tokens, num_return_sequences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreedy_output\u001b[39m(sentence, max_new_tokens, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m     greedy_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m greedy_output\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/transformers/generation/utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1702\u001b[0m         input_ids,\n\u001b[1;32m   1703\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/transformers/generation/utils.py:2579\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2579\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:928\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    923\u001b[0m         v\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     )\n\u001b[0;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseModelOutputWithPastAndCrossAttentions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_self_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_cross_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:8\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, last_hidden_state, past_key_values, hidden_states, attentions, cross_attentions)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/transformers/utils/generic.py:382\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m class_fields:\n\u001b[0;32m--> 382\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, field\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m             \u001b[38;5;28mself\u001b[39m[field\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_epochs=10\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_train, loss_val, acc_train, acc_val = fit(model,train, test, num_epochs, 40, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d29746b9-4b88-4105-8f74-285df342537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person work as a contractor, or a person who is a member of a trade union, is not required to register as an employee of the employer.\n",
      "\n",
      "(2) A person is required by this section to\n"
     ]
    }
   ],
   "source": [
    "out = greedy_output(sentences[0][1], 40)\n",
    "# logits = torch.cat([sc.softmax(-1) for sc in out.scores])\n",
    "print(tokenizer.decode(out.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022a5ce-f930-4dd8-97cc-97321cbd10ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
