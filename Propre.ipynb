{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabc677e-90cb-474a-a9af-06d55091782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prtissot/Documents/IA/ENV/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-16 09:08:49.025620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 09:08:49.025647: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 09:08:49.026002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-16 09:08:49.075853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 09:08:50.062780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "import torch, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82710fe4-62f3-4318-8461-32fc5b729195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "sentence = \"A gay man suffering from cancer must\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f10e66a-f06c-44ad-8700-6899ab86dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430824e7-22cc-4e56-9a8e-2b9dea96813e",
   "metadata": {},
   "source": [
    "## With pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644a9778-7a29-474e-90df-f5f52b9a3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759db776-9fe3-494a-ae96-fe51ff398034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gay man suffering from cancer must now be put up for adoption by his biological mother.\"\n",
      "\n",
      "The statement was made without disclosing the man's actual\n"
     ]
    }
   ],
   "source": [
    "text = generator(sentence, max_length=30, pad_token_id=50256, num_return_sequences=1)\n",
    "print(text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7338e-1811-4fc1-a688-06c77785ae34",
   "metadata": {},
   "source": [
    "## With generate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eb4c0-dcda-478d-9538-28ac371c5788",
   "metadata": {},
   "source": [
    "### Greedy output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a51f214-8419-4867-a0ad-af25ee5693d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "greedy_output = model.generate(\n",
    "    pad_token_id=50256,\n",
    "    **model_inputs, \n",
    "    max_new_tokens=40, \n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True, \n",
    "    no_repeat_ngram_size=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee772e4-0081-49fa-9f2c-c114cb265c7d",
   "metadata": {},
   "source": [
    "#### Output with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3a10bc-61a0-45c5-b126-1639980abe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be treated with the same care as a woman who suffers from a terminal illness.\n",
      "\n",
      "The bill, which was introduced by Rep. John Conyers Jr., D-Mich., would require that all"
     ]
    }
   ],
   "source": [
    "for sc in greedy_output.scores:\n",
    "    res = torch.argmax(sc)\n",
    "    print(tokenizer.decode(res), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a68d68-e429-4466-abb9-d62832de1d04",
   "metadata": {},
   "source": [
    "#### Output with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7cdedb-aefb-4197-a6dc-527965c43cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gay man suffering from cancer must be treated with the same care as a woman who suffers from a terminal illness.\n",
      "\n",
      "The bill, which was introduced by Rep. John Conyers Jr., D-Mich., would require that all\n"
     ]
    }
   ],
   "source": [
    "for g in greedy_output.sequences:\n",
    "    print(tokenizer.decode(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a0eab-2012-4e1a-862c-01f2671c43d7",
   "metadata": {},
   "source": [
    "### Bean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6702648-a41b-43bf-b95e-ab1c10f0d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "beam_output = model.generate(\n",
    "    pad_token_id = 50256,\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=3,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=1,\n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66d428a-341e-4368-8f2c-4ed897e3e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  32, 5650,  582, 7195,  422, 4890, 1276,  307, 5716,  588,  597,  584,\n",
      "         1048,   13,  198,  198,    1,   40, 1101,  407, 1016,  284, 6486,  284,\n",
      "          345,   13,  314, 1101,  655,  407, 6792,  351,  340,  553,  339,  531,\n",
      "           13,  366,   40,  836,  470,  760,  644,  284,  466,  546,  340]]), tensor([-1.3516]))\n"
     ]
    }
   ],
   "source": [
    "print(beam_output[:2]) # loss and logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d11141-bb5c-4bea-b463-de00e31ab402",
   "metadata": {},
   "source": [
    "#### Output with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c21be6-b945-4ef6-8f5f-ae55933e5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gay man suffering from cancer must be treated like any other person.\n",
      "\n",
      "\"I'm not going to lie to you. I'm just not comfortable with it,\" he said. \"I don't know what to do about it\n"
     ]
    }
   ],
   "source": [
    "for b in beam_output.sequences:\n",
    "    print(tokenizer.decode(b, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a8d91a-b2b4-45f1-aa6f-d1ca24a86924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  32, 5650,  582, 7195,  422, 4890, 1276,  307, 5716,  588,  597,  584,\n",
      "        1048,   13,  198,  198,    1,   40, 1101,  407, 1016,  284, 6486,  284,\n",
      "         345,   13,  314, 1101,  655,  407, 6792,  351,  340,  553,  339,  531,\n",
      "          13,  366,   40,  836,  470,  760,  644,  284,  466,  546,  340])\n"
     ]
    }
   ],
   "source": [
    "for b in beam_output.sequences:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33defb76-cd64-465d-8a0b-c88737f7de47",
   "metadata": {},
   "source": [
    "#### Output with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceba95c3-fc60-4d2a-849b-ec6e5d2ae2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be treated a the human person.\n",
      "\n",
      "\"I'm not just to be a I, I'm a going out that it,\" he said. \"I don not want to be a. It"
     ]
    }
   ],
   "source": [
    "# print(beam_output.scores[0].softmax(-1))\n",
    "for sc in beam_output.scores:\n",
    "    # sc = torch.nn.functional.softmax(sc,dim=1)\n",
    "    # print(sc.shape)\n",
    "    # sum = torch.prod(sc,0)\n",
    "    # print(sum.shape)\n",
    "    res = torch.argmax(sc[0,:])\n",
    "    # print(res)\n",
    "    # res = torch.argmax(sc)%sc.shape[1]\n",
    "    # print(res1)\n",
    "    # print(res)\n",
    "    # res = torch.argmax(sc)\n",
    "    print(tokenizer.decode(res, skip_special_tokens=True), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df7e19-20e1-45fa-9a10-4cdcdda036d3",
   "metadata": {},
   "source": [
    "### With model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "901a0d86-32e1-4abe-a131-16ba2277378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gay man suffering from cancer must be treated with the same care as a woman who suffers from a terminal illness.\n",
      "\n",
      "\n",
      "The bill, which was introduced by the Liberal Democrat MP for Stirling, John Bercow, would make it a crime for a person to discriminate against"
     ]
    }
   ],
   "source": [
    "sent_cpy = copy.copy(sentence)\n",
    "print(sent_cpy, end='')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        inputs = tokenizer(sent_cpy, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.softmax(-1).squeeze()\n",
    "        res = torch.argmax(logits[-1, :])\n",
    "        carac = tokenizer.decode(res)\n",
    "        sent_cpy += carac\n",
    "        print(carac,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fd681-96b3-47c3-924c-5f10e0997c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3551339-e584-496e-aad1-609f7894b91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
